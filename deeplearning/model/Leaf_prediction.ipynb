{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import resources\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms, models\n",
    "\n",
    "import json\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import copy\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'all_data'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/valid'\n",
    "test_dir = data_dir + '/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Define your transforms for the training, validation, and testing sets\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                           [0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Load the datasets with ImageFolder\n",
    "train_data = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "valid_data = datasets.ImageFolder(valid_dir, transform=train_transforms)\n",
    "test_data = datasets.ImageFolder(test_dir ,transform = test_transforms)\n",
    "\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "             train_data,\n",
    "             batch_size=batch_size, shuffle=True,\n",
    "             num_workers=num_workers)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data,\n",
    "             batch_size=batch_size, shuffle=True,\n",
    "             num_workers=num_workers)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build and train your network\n",
    "\n",
    "model = models.resnet34(pretrained=True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Sequential(nn.Linear(512, 256),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.2),\n",
    "                                 nn.Linear(256, 2),\n",
    "                                 nn.LogSoftmax(dim=1))\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.003)\n",
    "\n",
    "# move the model to GPU, if available\n",
    "train_on_gpu = torch.cuda.is_available()    \n",
    "    \n",
    "# move tensors to GPU if CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.224734 \tValidation Loss: 1.007366 \t accuracy:0.427031\n",
      "Epoch: 2 \tTraining Loss: 0.780965 \tValidation Loss: 0.603771 \t accuracy:0.652500\n",
      "Epoch: 3 \tTraining Loss: 0.610009 \tValidation Loss: 0.575012 \t accuracy:0.719375\n",
      "Epoch: 4 \tTraining Loss: 0.567159 \tValidation Loss: 0.540188 \t accuracy:0.699375\n",
      "Epoch: 5 \tTraining Loss: 0.528707 \tValidation Loss: 0.498585 \t accuracy:0.738438\n",
      "Epoch: 6 \tTraining Loss: 0.477685 \tValidation Loss: 0.469679 \t accuracy:0.799375\n",
      "Epoch: 7 \tTraining Loss: 0.445611 \tValidation Loss: 0.430836 \t accuracy:0.806719\n",
      "Epoch: 8 \tTraining Loss: 0.418546 \tValidation Loss: 0.416567 \t accuracy:0.818437\n",
      "Epoch: 9 \tTraining Loss: 0.410244 \tValidation Loss: 0.418120 \t accuracy:0.758906\n",
      "Epoch: 10 \tTraining Loss: 0.368434 \tValidation Loss: 0.388016 \t accuracy:0.842344\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 10\n",
    "\n",
    "\n",
    "\n",
    "best_model_weights = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    accuracy = 0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        ps = torch.exp(output).data\n",
    "        equality = (target.data == ps.max(1)[1])\n",
    "        accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "        \n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "    accuracy = accuracy /len(valid_loader)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\t accuracy:{:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss,accuracy))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        best_acc = accuracy\n",
    "        best_model_weights = copy.deepcopy(model.state_dict())\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(best_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test accuracy: 0.833834171295166\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    batch_accuracy = []\n",
    "    \n",
    "    for idx, (inputs, labels) in enumerate(test_loader):\n",
    "        \n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        outputs = model.forward(inputs)\n",
    "        _, predicted = outputs.max(dim=1)\n",
    "        equals = predicted == labels.data\n",
    "        #print(\"Batch accuracy (Size {}): {}\".format(batch_size, equals.float().mean()))\n",
    "        batch_accuracy.append(equals.float().mean().cpu().numpy())\n",
    "    mean_acc = np.mean(batch_accuracy)\n",
    "    print(\"Mean Test accuracy: {}\".format(mean_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save the checkpoint \n",
    "\n",
    "model.class_to_idx = train_data.class_to_idx\n",
    "model.cpu\n",
    "torch.save({'structure' :'resnet18',\n",
    "            'input_size': [3, 224, 224],\n",
    "            'state_dict':model.state_dict(),\n",
    "            'class_to_idx':model.class_to_idx},\n",
    "            'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a function that loads a checkpoint and rebuilds the model\n",
    "\n",
    "def load_model(path):\n",
    "    checkpoint = torch.load('checkpoint.pth')\n",
    "    structure = checkpoint['structure']\n",
    "    model = models.resnet34()\n",
    "    \n",
    "    classifier = nn.Sequential(nn.Linear(512, 256),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.2),\n",
    "                                 nn.Linear(256, 2),\n",
    "                                 nn.LogSoftmax(dim=1))\n",
    "    \n",
    "    \n",
    "    model.fc = classifier\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    return model, checkpoint['class_to_idx']\n",
    "    \n",
    "loaded_model, class_to_idx = load_model('my_checkpoint.pth')\n",
    "idx_to_class = { v : k for k,v in class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
    "        returns an Numpy array\n",
    "    '''\n",
    "    \n",
    "    # TODO: Process a PIL image for use in a PyTorch model\n",
    "    img_pil = Image.open(image)\n",
    "   \n",
    "    adjustments = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    img_tensor = adjustments(img_pil)\n",
    "    \n",
    "    return img_tensor\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, ax=None, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    # PyTorch tensors assume the color channel is the first dimension\n",
    "    # but matplotlib assumes is the third dimension\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    # Undo preprocessing\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    \n",
    "    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
    "    image = np.clip(image, 0, 1)\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_path, model, topk=1):\n",
    "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
    "    '''\n",
    "    \n",
    "    # TODO: Implement the code to predict the class from an image file\n",
    "    model.to('cuda:0')\n",
    "    img_torch = process_image(image_path)\n",
    "    img_torch = img_torch.unsqueeze_(0)\n",
    "    img_torch = img_torch.float()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.forward(img_torch.cuda())\n",
    "        \n",
    "    probability = F.softmax(output.data,dim=1)\n",
    "    \n",
    "    return probability.topk(topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6778]], device='cuda:0')\n",
      "tensor([[0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "img = ('./Leaf_data/test/Negative/30-03-2019_15-19-25.jpg')\n",
    "val1, val2 = predict(img,loaded_model)\n",
    "print(val1)\n",
    "print(val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Negative', 1: 'Positive'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_class[val2.cpu().numpy()[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[73 17]\n",
      " [18 46]]\n",
      "[81.11111111 71.875     ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Initialize the prediction and label lists(tensors)\n",
    "predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, classes) in enumerate(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        classes = classes.to(device)\n",
    "        outputs = loaded_model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        # Append batch prediction results\n",
    "        predlist=torch.cat([predlist,preds.view(-1).cpu()])\n",
    "        lbllist=torch.cat([lbllist,classes.view(-1).cpu()])\n",
    "\n",
    "# Confusion matrix\n",
    "conf_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "print(conf_mat)\n",
    "\n",
    "# Per-class accuracy\n",
    "class_accuracy=100*conf_mat.diagonal()/conf_mat.sum(1)\n",
    "print(class_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7727272727272727"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(lbllist.numpy(), predlist.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Good Leaf','Bad Leaf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=False):\n",
    "\n",
    "\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(cm=conf_mat,target_names=class_names,cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
